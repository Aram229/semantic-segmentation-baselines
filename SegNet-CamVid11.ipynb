{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import pandas as pd\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from src import camvid\n",
    "from src import segnet\n",
    "from src.utils import history_to_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_table('11_class.txt', sep=r'\\s+', names=['og', 'new'], index_col='og')['new'].to_dict()\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size to reshape images to before transformation\n",
    "target_size = (360, 480)\n",
    "# the size to crop images to\n",
    "crop_size = (352, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all crop dimensions must be divisible by this value due \n",
    "# to the requirement of equal shapes between downsampling \n",
    "# outputs and upsampling inputs imposed by the concatenation\n",
    "# in skip link connections\n",
    "divisible_by = int(2**5)\n",
    "# iterate over all the crop dimensions\n",
    "for dim in crop_size:\n",
    "    # raise error if the dimension has a remainder when divided\n",
    "    if dim % divisible_by:\n",
    "        f = 'crop dimension ({}) must be divisible by {}'\n",
    "        f = f.format(dim, divisible_by)\n",
    "        raise ValueError(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid11 = camvid.CamVid(mapping=mapping, target_size=target_size, crop_size=crop_size, batch_size=4)\n",
    "generators = camvid11.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next X, y training tuple\n",
    "X, y = next(generators['train'])\n",
    "# transform the onehot vector to an image\n",
    "y = camvid11.unmap(y)\n",
    "# plot the images\n",
    "camvid.plot(X=X[0], y=y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model for the image shape and number of labels\n",
    "model = segnet.build_segnet((*crop_size, 3), camvid11.n, \n",
    "    label_names=camvid11.discrete_to_label_map,\n",
    "    class_weights=camvid11.class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit the model with the data. divide the steps per epoch by the \n",
    "# batch size (which is 3 in this case)\n",
    "history = model.fit_generator(generators['train'],\n",
    "    epochs=100,\n",
    "    steps_per_epoch=int(367 / 4),\n",
    "    validation_data=generators['val'],\n",
    "    validation_steps=int(101 / 4),\n",
    "    callbacks=[EarlyStopping(monitor='loss', patience=10), PlotLossesKeras()],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_to_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate_generator(generators['test'], steps=int(233 / 4))\n",
    "names = model.metrics_names\n",
    "pd.DataFrame(metrics, names, columns=['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['train'])\n",
    "y = camvid11.unmap(y)\n",
    "p = model.predict(X)\n",
    "p = camvid11.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[1], y=y[1], y_pred=p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[2], y=y[2], y_pred=p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[3], y=y[3], y_pred=p[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['val'])\n",
    "y = camvid11.unmap(y)\n",
    "p = model.predict(X)\n",
    "p = camvid11.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[1], y=y[1], y_pred=p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[2], y=y[2], y_pred=p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[3], y=y[3], y_pred=p[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['test'])\n",
    "y = camvid11.unmap(y)\n",
    "p = model.predict(X)\n",
    "p = camvid11.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[1], y=y[1], y_pred=p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[2], y=y[2], y_pred=p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[3], y=y[3], y_pred=p[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights('models/SegNet-CamVid11.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
