{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import pandas as pd\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from src import camvid\n",
    "from src import tiramisu\n",
    "from src.utils import history_to_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size to reshape images to before transformation\n",
    "target_size = (360, 480)\n",
    "# the size to crop images to for coarse training\n",
    "coarse_crop = (224, 224)\n",
    "# the size to crop images to for fine tune training\n",
    "fine_crop = (352, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all crop dimensions must be divisible by this value due \n",
    "# to the requirement of equal shapes between downsampling \n",
    "# outputs and upsampling inputs imposed by the concatenation\n",
    "# in skip link connections\n",
    "divisible_by = int(2**5)\n",
    "# iterate over all the crop dimensions\n",
    "for dim in coarse_crop + fine_crop:\n",
    "    # raise error if the dimension has a remainder when divided\n",
    "    if dim % divisible_by:\n",
    "        f = 'crop dimension ({}) must be divisible by {}'\n",
    "        f = f.format(dim, divisible_by)\n",
    "        raise ValueError(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid32 = camvid.CamVid(target_size=target_size, crop_size=coarse_crop)\n",
    "generators = camvid32.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next X, y training tuple\n",
    "X, y = next(generators['training'])\n",
    "# transform the onehot vector to an image\n",
    "y = camvid32.unmap(y)\n",
    "# plot the images\n",
    "camvid.plot(X=X[0], y=y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model for the image shape and number of labels\n",
    "model = tiramisu.build_tiramisu((*coarse_crop, 3), camvid32.n,\n",
    "    label_names=camvid32.discrete_to_label_map,\n",
    "    learning_rate=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit the model with the data. divide the steps per epoch by the \n",
    "# batch size (which is 3 in this case)\n",
    "history = model.fit_generator(generators['training'],\n",
    "    epochs=50,\n",
    "    steps_per_epoch=int(491 / 3),\n",
    "    validation_data=generators['validation'],\n",
    "    validation_steps=int(210 / 3),\n",
    "    callbacks=[\n",
    "        LearningRateScheduler(lambda _, lr: 0.995 * lr),\n",
    "        EarlyStopping(monitor='val_acc', patience=100),\n",
    "        PlotLossesKeras(),\n",
    "    ],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_to_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate_generator(generators['validation'], steps=210)\n",
    "names = model.metrics_names\n",
    "pd.DataFrame(metrics, names, columns=['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['training'])\n",
    "y = camvid32.unmap(y)\n",
    "p = model.predict(X)\n",
    "p = camvid32.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[1], y=y[1], y_pred=p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[2], y=y[2], y_pred=p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['validation'])\n",
    "y = camvid32.unmap(y)\n",
    "p = model.predict(X)\n",
    "p = camvid32.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[1], y=y[1], y_pred=p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[2], y=y[2], y_pred=p[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights('models/Tiramisu103-CamVid32-coarse.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Session\n",
    "\n",
    "remove the current model from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid32 = camvid.CamVid(target_size=target_size, crop_size=fine_crop, batch_size=1)\n",
    "generators = camvid32.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the next X, y training tuple\n",
    "X, y = next(generators['training'])\n",
    "# transform the onehot vector to an image\n",
    "y = camvid32.unmap(y)\n",
    "# plot the images\n",
    "camvid.plot(X=X[0], y=y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model for the image shape and number of labels\n",
    "tune_model = tiramisu.build_tiramisu((*fine_crop, 3), camvid32.n,\n",
    "    label_names=camvid32.discrete_to_label_map,\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "# load the weights from the coarsely trained model\n",
    "tune_model.load_weights('./models/Tiramisu103-CamVid32-coarse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit the model with the data. use a small max queue size to \n",
    "# prevent an OOM error due to large image size\n",
    "history = tune_model.fit_generator(generators['training'],\n",
    "    epochs=50,\n",
    "    steps_per_epoch=491,\n",
    "    validation_data=generators['validation'],\n",
    "    validation_steps=210,\n",
    "    callbacks=[\n",
    "        LearningRateScheduler(lambda _, lr: 0.995 * lr),\n",
    "        EarlyStopping(monitor='val_acc', patience=50),\n",
    "        PlotLossesKeras(),\n",
    "    ],\n",
    "    verbose=0,\n",
    "    max_queue_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_to_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tune_model.evaluate_generator(generators['validation'], steps=210)\n",
    "names = tune_model.metrics_names\n",
    "pd.DataFrame(metrics, names, columns=['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['training'])\n",
    "y = camvid32.unmap(y)\n",
    "p = tune_model.predict(X)\n",
    "p = camvid32.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['validation'])\n",
    "y = camvid32.unmap(y)\n",
    "p = tune_model.predict(X)\n",
    "p = camvid32.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "tune_model.save_weights('models/Tiramisu103-CamVid32-fine.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
