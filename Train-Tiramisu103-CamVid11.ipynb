{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import pandas as pd\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from src import camvid\n",
    "from src import tiramisu\n",
    "from src.utils import history_to_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_table('11_class.txt', sep=r'\\s+', names=['og', 'new'], index_col='og')['new'].to_dict()\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "Because of the skip connections between the down-sampling and up-sampling sides of the network, input image size must be divisible by 32 (2^5). This is because of the max pooling / transpose convolution layers on the down-sampling and up-sampling branch of the network respectively. There are 5 of these units each with 2x2 kernel size and stride 2. Thus, the input must be divisible by 2^5 to ensure the outputs along the down-sampling side of the net can concatenate with the up-sampling side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size to reshape images to before transformation\n",
    "target_size = (360, 480)\n",
    "# the size to crop images to for coarse training\n",
    "coarse_crop = (224, 224)\n",
    "# the size to crop images to for fine tune training\n",
    "fine_crop = (352, 480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Tuning\n",
    "\n",
    "train on random crops of size 224 x 224 (images are also random flipped about x and y axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid11 = camvid.CamVid(mapping=mapping, target_size=target_size, crop_size=coarse_crop)\n",
    "generators = camvid11.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next X, y training tuple\n",
    "X, y = next(generators['training'])\n",
    "# transform the onehot vector to an image\n",
    "y = camvid11.unmap(y)\n",
    "# plot the images\n",
    "camvid.plot(X=X[0], y=y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model for the image shape and number of labels\n",
    "model = tiramisu.build_tiramisu((*coarse_crop, 3), camvid11.n,\n",
    "    label_names=camvid11.discrete_to_label_map,\n",
    "    learning_rate=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit the model with the data. divide the steps per epoch by the \n",
    "# batch size (which is 3 in this case)\n",
    "history = model.fit_generator(generators['training'],\n",
    "    epochs=50,\n",
    "    steps_per_epoch=int(491 / 3),\n",
    "    validation_data=generators['validation'],\n",
    "    validation_steps=int(210 / 3),\n",
    "    callbacks=[\n",
    "        LearningRateScheduler(lambda _, lr: 0.995 * lr),\n",
    "        EarlyStopping(monitor='val_acc', patience=100),\n",
    "        PlotLossesKeras(),\n",
    "    ],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_to_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate_generator(generators['validation'], steps=210)\n",
    "names = model.metrics_names\n",
    "pd.DataFrame(metrics, names, columns=['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['training'])\n",
    "y = camvid11.unmap(y)\n",
    "p = model.predict(X)\n",
    "p = camvid11.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[1], y=y[1], y_pred=p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[2], y=y[2], y_pred=p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['validation'])\n",
    "y = camvid11.unmap(y)\n",
    "p = model.predict(X)\n",
    "p = camvid11.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[1], y=y[1], y_pred=p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[2], y=y[2], y_pred=p[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights('models/Tiramisu103-CamVid11-coarse.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Session\n",
    "\n",
    "remove the current model from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "Train on larger crops (because the full size image results in a OutOfMemory error on the P100). max queue size of the fit generator is also reduced (resulting in a slowdown) to accomodate these larger images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid11 = camvid.CamVid(mapping=mapping, target_size=target_size, crop_size=fine_crop, batch_size=1)\n",
    "generators = camvid11.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the next X, y training tuple\n",
    "X, y = next(generators['training'])\n",
    "# transform the onehot vector to an image\n",
    "y = camvid11.unmap(y)\n",
    "# plot the images\n",
    "camvid.plot(X=X[0], y=y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model for the image shape and number of labels\n",
    "tune_model = tiramisu.build_tiramisu((*fine_crop, 3), camvid11.n,\n",
    "    label_names=camvid11.discrete_to_label_map,\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "# load the weights from the coarsely trained model\n",
    "tune_model.load_weights('./models/Tiramisu103-CamVid11-coarse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit the model with the data. use a small max queue size to \n",
    "# prevent an OOM error due to large image size\n",
    "history = tune_model.fit_generator(generators['training'],\n",
    "    epochs=50,\n",
    "    steps_per_epoch=491,\n",
    "    validation_data=generators['validation'],\n",
    "    validation_steps=210,\n",
    "    callbacks=[\n",
    "        LearningRateScheduler(lambda _, lr: 0.995 * lr),\n",
    "        EarlyStopping(monitor='val_acc', patience=50),\n",
    "        PlotLossesKeras(),\n",
    "    ],\n",
    "    verbose=0,\n",
    "    max_queue_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_to_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tune_model.evaluate_generator(generators['validation'], steps=210)\n",
    "names = tune_model.metrics_names\n",
    "pd.DataFrame(metrics, names, columns=['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['training'])\n",
    "y = camvid11.unmap(y)\n",
    "p = tune_model.predict(X)\n",
    "p = camvid11.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(generators['validation'])\n",
    "y = camvid11.unmap(y)\n",
    "p = tune_model.predict(X)\n",
    "p = camvid11.unmap(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camvid.plot(X=X[0], y=y[0], y_pred=p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "tune_model.save_weights('models/Tiramisu103-CamVid11-fine.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
